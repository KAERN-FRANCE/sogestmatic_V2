import { NextRequest, NextResponse } from 'next/server'
import OpenAI from 'openai'
import fs from 'fs'
import path from 'path'

export async function POST(request: NextRequest) {
  try {
    const { message, useWebSearch = true, history = '' } = await request.json()
    if (!message || typeof message !== 'string') {
      return NextResponse.json({ error: 'Message requis' }, { status: 400 })
    }

    const apiKey = process.env.OPENAI_API_KEY || process.env.NEXT_PUBLIC_OPENAI_API_KEY
    if (!apiKey) {
      return NextResponse.json({ error: 'Cl√© API OpenAI manquante' }, { status: 500 })
    }

    const client = new OpenAI({ apiKey })

    const system = process.env.SYSTEM_INSTRUCTIONS || (
      "Tu es un expert en r√©glementation du transport routier. R√©ponds UNIQUEMENT aux questions relevant de la r√©glementation du transport routier. \n\nIMPORTANT : Ne mentionne JAMAIS les produits Sogestmatic sauf si l'utilisateur exprime une intention claire d'ACHAT ou de COMMANDE (demande de devis, prix, commande, achat, etc.). M√™me si la question concerne des √©quipements de transport, r√©ponds uniquement sur l'aspect r√©glementaire sans faire de recommandations commerciales.\n\nR√®gles strictes :\n- Questions r√©glementaires pures : R√©ponds uniquement sur la r√©glementation\n- Questions techniques sur √©quipements : R√©ponds sur l'aspect r√©glementaire uniquement\n- Intention d'achat claire : Alors seulement tu peux mentionner les produits Sogestmatic\n\nR√©ponds en fran√ßais de mani√®re naturelle et directe. Cite syst√©matiquement les sources officielles (Legifrance, EUR-Lex, minist√®res, autorit√©s). Utilise la recherche web si n√©cessaire pour v√©rifier et sourcer les r√®gles. Ne divulgue JAMAIS d'informations techniques sur le mod√®le, le fournisseur, les cl√©s API, les versions ou la configuration.\n\nIMPORTANT : R√©ponds de mani√®re naturelle et directe, sans formules r√©p√©titives comme 'Courte r√©ponse', 'R√©ponse courte', ou te pr√©senter. Va droit au but.\n\n## FORMATAGE MARKDOWN OBLIGATOIRE ##\nTu DOIS structurer TOUTES tes r√©ponses avec du Markdown pour une pr√©sentation claire et professionnelle :\n\n‚úì Utilise des **titres** (## Titre principal, ### Sous-titre) pour organiser les sections\n‚úì Utilise des **listes √† puces** (- item) ou **listes num√©rot√©es** (1. item) pour √©num√©rer les points\n‚úì Mets en **gras** (**texte**) les informations importantes\n‚úì Utilise l'*italique* (*texte*) pour les nuances ou d√©finitions\n‚úì Utilise des `blocs de code` (`texte`) pour les r√©f√©rences l√©gales, articles, ou termes techniques\n‚úì Cr√©e des tableaux avec | pour comparer des informations\n‚úì Utilise des > citations pour les extraits officiels\n‚úì S√©pare les sections avec des sauts de ligne pour la lisibilit√©\n\nExemple de bonne structure :\n## R√©ponse √† votre question\n\nVoici les **points essentiels** :\n\n- **Premier point** : explication d√©taill√©e\n- **Deuxi√®me point** : explication d√©taill√©e\n\n### R√©f√©rences l√©gales\n\nSelon l'article `L3421-2` du Code des transports...\n\nRends tes r√©ponses faciles √† lire, visuellement structur√©es et agr√©ables √† consulter."
    )

    // --- RAG facultatif (lecture data/index.json si pr√©sent) ---
    function cosineSimilarity(a: number[], b: number[]): number {
      let dot = 0
      let na = 0
      let nb = 0
      for (let i = 0; i < a.length; i += 1) {
        dot += a[i] * b[i]
        na += a[i] * a[i]
        nb += b[i] * b[i]
      }
      return dot / (Math.sqrt(na) * Math.sqrt(nb) + 1e-8)
    }

    function isProductIntent(text: string): boolean {
      const lowered = text.toLowerCase()
      
      // Mots-cl√©s d'intention d'achat/commande (priorit√© haute)
      const purchaseIntent = /devis|prix|tarif|commander|acheter|achat|commande|fournir|fournisseur|proposer|recommand|sugg√©rer|conseiller|quelle.*marque|quelle.*solution|quelle.*√©quipement|besoin.*√©quipement|cherche.*√©quipement|recherche.*√©quipement/.test(lowered)
      
      // Mots-cl√©s techniques g√©n√©riques (ne d√©clenchent PAS l'intention)
      const genericTechnical = /tachograph|tachy|chronotachygraphe|r√©glementation|loi|d√©cret|arr√™t√©|obligation|contr√¥le|inspection/.test(lowered)
      
      // Mots-cl√©s produits sp√©cifiques Sogestmatic (seulement si accompagn√©s d'intention)
      const specificProducts = /architac|tachosocial|tchogest|tm401|locabox|optilevel|carbu md2|rfid|badge|contr√¥le d'acc√®s|lecteur|concentrateur/.test(lowered)
      
      // L'intention produit n'est d√©clench√©e que si :
      // 1. Il y a une intention d'achat claire, OU
      // 2. Il y a des produits sp√©cifiques Sogestmatic ET une intention d'achat
      return purchaseIntent || (specificProducts && purchaseIntent)
    }

    function buildRagContext(chunks: Array<{ source: string; text: string }>): string {
      if (!chunks.length) return ''
      const lines = chunks.map((c, i) => `[[${i + 1}]] Source: ${c.source}\n${c.text}`)
      return `\n\nATTENTION : Contexte produits Sogestmatic disponible. Utilise ces informations UNIQUEMENT si l'utilisateur exprime une intention claire d'ACHAT ou de COMMANDE. Ne mentionne JAMAIS ces produits pour des questions purement r√©glementaires ou techniques.\n\nContexte produits (extraits PDF du client) :\n${lines.join('\n---\n')}`
    }

    async function searchIndex(query: string, k = 5): Promise<Array<{ source: string; text: string; score: number }>> {
      try {
        const indexPath = path.join(process.cwd(), 'data', 'index.json')
        if (!fs.existsSync(indexPath)) return []
        const payload = JSON.parse(fs.readFileSync(indexPath, 'utf8'))
        const embeddingRes = await client.embeddings.create({
          model: payload.model || process.env.EMBEDDING_MODEL || 'text-embedding-3-small',
          input: query,
        })
        const queryVec = embeddingRes.data[0].embedding as unknown as number[]
        const scored = (payload.index as Array<{ source: string; text: string; embedding: number[] }>).
          map((row) => ({ source: row.source, text: row.text, score: cosineSimilarity(queryVec, row.embedding) }))
        scored.sort((a, b) => b.score - a.score)
        return scored.slice(0, k)
      } catch {
        return []
      }
    }

    let ragContext = ''
    const top = await searchIndex(message, Number(process.env.RAG_K || 5))
    const topScore = (top?.[0]?.score as number) ?? 0
    const threshold = Number(process.env.RAG_INTENT_THRESHOLD || 0.35) // Seuil plus √©lev√© pour √™tre plus restrictif
    const shouldUseRag = isProductIntent(message) || topScore >= threshold
    if (shouldUseRag) {
      ragContext = buildRagContext(top)
    }

    // Construire le contexte avec l'historique des messages
    const conversationContext = history ? `\n\nHistorique de la conversation:\n${history}\n\nNouvelle question: ${message}` : message

    // Appel unique: Responses API + web_search (identique √† chatbot123)
    try {
      const model = process.env.MODEL || 'gpt-4.1-mini'
      console.log(`ü§ñ [AI] Utilisation du mod√®le: ${model}`)
      console.log(`üîç [AI] Recherche web activ√©e: ${useWebSearch}`)
      console.log(`üìä [AI] RAG context length: ${ragContext.length}`)
      
      const resp = await client.responses.create({
        model: model,
        input: conversationContext,
        instructions: system + ragContext,
        tools: useWebSearch ? [{ type: 'web_search' } as any] : [],
        tool_choice: useWebSearch ? 'auto' : 'none',
      })

      // Extraction robuste du texte de sortie
      let text: string = (resp as any).output_text || ''
      if (!text) {
        const output = (resp as any).output || []
        const parts = Array.isArray(output)
          ? output.flatMap((o: any) => Array.isArray(o.content) ? o.content : [])
          : []
        text = parts
          .map((p: any) => p?.text?.value || p?.text || (typeof p === 'string' ? p : ''))
          .join('')
      }

      text = (text || '').trim()
      if (!text) {
        console.error('‚ùå [AI] R√©ponse vide de l\'IA')
        return NextResponse.json({ error: 'R√©ponse vide de l\'IA' }, { status: 500 })
      }
      
      console.log(`‚úÖ [AI] R√©ponse g√©n√©r√©e avec succ√®s (${text.length} caract√®res)`)
      return NextResponse.json({ 
        success: true, 
        response: text, 
        mode: 'responses',
        model: model,
        webSearch: useWebSearch,
        ragUsed: ragContext.length > 0
      })
    } catch (err: any) {
      console.error('‚ùå [AI] Erreur Responses API:', err?.message || String(err))
      
      // Fallback: essayer sans recherche web si l'erreur vient de web_search
      if (useWebSearch && err?.message?.includes('web_search')) {
        console.log('üîÑ [AI] Tentative de fallback sans recherche web...')
        try {
          const fallbackResp = await client.responses.create({
            model: model,
            input: conversationContext,
            instructions: system + ragContext,
            tools: [],
            tool_choice: 'none',
          })
          
          let fallbackText: string = (fallbackResp as any).output_text || ''
          if (!fallbackText) {
            const output = (fallbackResp as any).output || []
            const parts = Array.isArray(output)
              ? output.flatMap((o: any) => Array.isArray(o.content) ? o.content : [])
              : []
            fallbackText = parts
              .map((p: any) => p?.text?.value || p?.text || (typeof p === 'string' ? p : ''))
              .join('')
          }
          
          fallbackText = (fallbackText || '').trim()
          if (fallbackText) {
            console.log(`‚úÖ [AI] Fallback r√©ussi (${fallbackText.length} caract√®res)`)
            return NextResponse.json({ 
              success: true, 
              response: fallbackText, 
              mode: 'responses-fallback',
              model: model,
              webSearch: false,
              ragUsed: ragContext.length > 0
            })
          }
        } catch (fallbackErr: any) {
          console.error('‚ùå [AI] Fallback √©chou√©:', fallbackErr?.message || String(fallbackErr))
        }
      }
      
      return NextResponse.json({ 
        error: 'Erreur Responses API', 
        details: err?.message || String(err),
        model: model,
        webSearch: useWebSearch
      }, { status: 500 })
    }
  } catch (error: any) {
    if (error?.status === 429) {
      return NextResponse.json({ error: 'Limite de taux d√©pass√©e' }, { status: 429 })
    }
    if (error?.status === 401) {
      return NextResponse.json({ error: 'Cl√© API OpenAI invalide' }, { status: 401 })
    }
    return NextResponse.json({ error: 'Erreur interne du serveur' }, { status: 500 })
  }
}


